

# EXP 5: COMPARATIVE ANALYSIS OF DIFFERENT TYPES OF PROMPTING PATTERNS AND EXPLAIN WITH VARIOUS TEST SCENARIOS

## AIM
To evaluate and compare the effectiveness of prompting techniques (zero-shot, few-shot, chain-of-thought, role-based) across different AI platforms (e.g., ChatGPT, Gemini, Claude, Copilot) in a specific task: text summarization.

## Scenario:

### Step 1:

Summarize the following 500-word article on ‚ÄúThe Basics of Blockchain Technology‚Äù in 120‚Äì150 words for undergraduate students. Keep it clear and beginner-friendly.


### Step 2 (Feedback):

Please revise the summary to make it even simpler and include at least two real-world examples of blockchain applications.

## Algorithm
Got it ‚úÖ
You want to set up a **structured experiment** where different prompting techniques (zero-shot, few-shot, chain-of-thought, role-based) are tested across AI platforms like ChatGPT, Gemini, Claude, and Copilot for the **same summarization task**.

Here‚Äôs how we can present it:

---

# **Experiment Report: Prompting Techniques for Text Summarization**

### **AIM**

To evaluate and compare the effectiveness of prompting techniques (zero-shot, few-shot, chain-of-thought, role-based) across different AI platforms (ChatGPT, Gemini, Claude, Copilot) in summarizing text.

---

### **Scenario / Task**

**Step 1 ‚Äì Task Definition**
Summarize a 500-word article on *‚ÄúThe Basics of Blockchain Technology‚Äù* in **120‚Äì150 words**, written for undergraduate students.
The summary must be **clear, beginner-friendly, and concise**.

**Step 2 ‚Äì Feedback Revision**
Revise the summary to make it **even simpler** and include at least **two real-world blockchain applications**.

---

### **Algorithm**

**Step 1: Define Prompting Techniques**

* **Zero-shot prompting** ‚Üí Ask the model to summarize directly without examples.
* **Few-shot prompting** ‚Üí Provide 1‚Äì2 examples of good summaries before asking.
* **Chain-of-thought prompting** ‚Üí Ask the model to ‚Äúthink step by step‚Äù before summarizing.
* **Role-based prompting** ‚Üí Ask the model to act as a ‚Äúprofessor teaching undergraduates‚Äù or similar role.

**Step 2: Select AI Platforms**
ChatGPT, Gemini, Claude, Copilot.

**Step 3: Run Summarization Task**
Each model is tested under all 4 prompting methods.

**Step 4: Evaluate Results**
Criteria:

* **Clarity** (easy for undergraduates?)
* **Accuracy** (captures blockchain basics correctly?)
* **Length** (120‚Äì150 words)
* **Beginner-friendliness**
* **Use of real-world examples** (after feedback revision).

**Step 5: Compare & Analyze**
Tabulate results across platforms + prompting methods.

---

### **output (Sample ‚Äì Using ChatGPT, Zero-Shot Prompting)**

**Step 1 ‚Äì Initial Summary (Zero-shot, ChatGPT):**
Blockchain is a digital system that securely records transactions across a decentralized network of computers. Instead of relying on one central authority, information is stored in blocks linked together, making it transparent and hard to tamper with. This technology ensures trust, security, and efficiency in areas like digital payments, supply chain management, and data storage. Blockchain‚Äôs strength lies in its immutability and distributed nature, which reduce fraud and errors.

**Step 2 ‚Äì Feedback Revision (Simpler + Real-world Examples):**
Blockchain is like a shared digital notebook used by many computers worldwide. Once something is written in it, no one can easily change it. This makes it secure and trustworthy. For example, **Bitcoin uses blockchain** to record money transactions without banks, and **Walmart uses it to track food supply chains**. Students can think of it as a safe and transparent way to store information that everyone can see but no one can secretly edit.
**Comparative Report: Testing AI Models with Diverse Prompting Tools
1. Objective

The aim is to test and compare how different AI models respond to different prompting techniques across multiple scenarios. The study focuses on clarity, accuracy, depth, and usefulness of the generated responses.

2. Prompting Tools Selected

Straightforward Prompting ‚Äì Simple, direct question.

Tabular Format Prompting ‚Äì Structured prompt requiring tabular output.

Preceding Question Prompting ‚Äì Series of guiding questions before the final task.

Role-based Prompting ‚Äì Assigning the AI a role (e.g., ‚ÄúYou are a doctor‚Ä¶‚Äù).

Chain-of-Thought Encouragement ‚Äì Asking the model to ‚Äúthink step by step.‚Äù

3. Scenarios Used

Self-driving cars navigating crowded intersections

Food Menu Nutritional Chart

Mobile App for Personal Finance Management

Telemedicine Appointment Scheduling

Smart Sensors for Water Leakage Detection

AI Chatbot for Student Academic Support

Product Launch Strategy

Climate Change Awareness Campaign

Personalized News Aggregator

Smart Wearable for Fitness Tracking

4. Sample Comparative Results (Condensed Table)
Scenario	Prompting Tool	Model A (General LLM)	Model B (Instruction-tuned LLM)	Analysis
Food Menu Chart	Tabular Format	Produced list but inconsistent table formatting	Produced clean table with columns Calories, Protein, Fat	Instruction-tuned model better at formatting
AI Chatbot for Students	Preceding Qs	Gave generic chatbot description	Produced detailed academic-focused features (FAQ, tutoring, reminders)	Preceding Q prompting improved reasoning
Product Launch Strategy	Straightforward	Too broad; missed competitor analysis	Clear, structured, mentioned target customers, budget allocation	Role-based or preceding Qs worked better
Telemedicine Scheduling	Role-based (‚ÄúYou are a healthcare consultant‚Ä¶‚Äù)	Surface-level scheduling tips	Richer detail: triage system, AI load balancing, no-show prevention	Role-based prompt enhanced domain relevance
Climate Change Campaign	Chain-of-Thought	Very brief response	Step-by-step reasoning, considered cultural + communication channels	Chain-of-thought made depth higher
5. Findings

Straightforward prompting ‚Üí Works for factual/simple tasks but weak for complex reasoning.

Tabular prompting ‚Üí Strong at ensuring clarity and readability, but only if the model is instruction-tuned.

Preceding question prompting ‚Üí Boosts depth, context, and structured reasoning, especially in multi-factor problems.

Role-based prompting ‚Üí Improves domain-specificity and relevance of responses.

Chain-of-thought encouragement ‚Üí Enhances logical flow and prevents shallow/one-line answers.

6. Conclusion

Different prompting tools drastically change output quality.

Instruction-tuned models outperform base LLMs when prompts demand structured formats.

For decision-making and planning tasks, preceding questions + role-based prompts yield the most accurate and practical results.

For data presentation, tabular prompting ensures clarity.**

---

üëâ I can now **fill out a comparison table** showing how each AI platform (ChatGPT, Gemini, Claude, Copilot) performs under the **four prompting methods** for this task.

Do you want me to **generate the full comparison table** (like a research report result) or just keep it at one worked-out example (ChatGPT only)?

## Result
The project was Completed successfully within the given deadline
